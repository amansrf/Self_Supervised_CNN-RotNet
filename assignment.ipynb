{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-07 22:46:20.270441: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-07 22:46:20.270496: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-07 22:46:20.270501: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from typing import Any\n",
    "\n",
    "from dataloader import load_data\n",
    "from RotNet import rotnet_constructor\n",
    "from PredNet import prednet_constructor\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import flax.linen as nn\n",
    "from flax import traverse_util\n",
    "from flax.core.frozen_dict import freeze\n",
    "from flax.training import train_state, checkpoints\n",
    "\n",
    "\n",
    "import optax\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cifar10 image shape\n",
    "CIFAR10_INPUT_SHAPE = (1, 32, 32, 3)\n",
    "\n",
    "class TrainState(train_state.TrainState):\n",
    "    batch_stats: Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss_(logits, labels, num_classes=10):\n",
    "    \"\"\"\n",
    "    Define loss: https://flax.readthedocs.io/en/latest/getting_started.html#define-loss\n",
    "    \"\"\"\n",
    "    labels_onehot = jax.nn.one_hot(labels, num_classes=num_classes)\n",
    "    return optax.softmax_cross_entropy(logits=logits, labels=labels_onehot).mean()\n",
    "cross_entropy_loss = jax.jit(cross_entropy_loss_, static_argnums=2)\n",
    "\n",
    "def compute_metrics_(logits, labels, num_classes):\n",
    "    \"\"\"\n",
    "    Metric computation: https://flax.readthedocs.io/en/latest/getting_started.html#metric-computation\n",
    "    \"\"\"\n",
    "    loss = cross_entropy_loss(logits=logits, labels=labels, num_classes=num_classes)\n",
    "    accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
    "    metrics = {\"loss\": loss, \"accuracy\": accuracy}\n",
    "    return metrics\n",
    "compute_metrics = jax.jit(compute_metrics_, static_argnums=2)\n",
    "\n",
    "def train_batch_(state, images, labels, num_classes=10):\n",
    "    \"\"\"\n",
    "    Training step: https://flax.readthedocs.io/en/latest/getting_started.html#training-step\n",
    "    \"\"\"\n",
    "    def loss_fn(params):\n",
    "        logits, updates = state.apply_fn(\n",
    "            {\"params\": params, \"batch_stats\": state.batch_stats}, images, mutable=[\"batch_stats\"], train=True\n",
    "        )\n",
    "        loss = cross_entropy_loss(logits=logits, labels=labels, num_classes=num_classes)\n",
    "        return loss, (logits, updates)\n",
    "\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (_, (logits, updates)), grads = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    state = state.replace(batch_stats=updates[\"batch_stats\"])\n",
    "    metrics = compute_metrics(logits=logits, labels=labels, num_classes=num_classes)\n",
    "    return state, metrics\n",
    "train_batch = jax.jit(train_batch_, static_argnums=3)\n",
    "\n",
    "def train_epoch(state, dataloader, num_classes=10):\n",
    "    \"\"\"\n",
    "    Train function: https://flax.readthedocs.io/en/latest/getting_started.html#train-function\n",
    "    \"\"\"\n",
    "    batch_metrics = []\n",
    "    for images, labels in dataloader:\n",
    "        state, metrics = train_batch(state, images, labels, num_classes=num_classes)\n",
    "        batch_metrics.append(metrics)\n",
    "    batch_metrics_np = jax.device_get(batch_metrics)\n",
    "    epoch_metrics_np = {\n",
    "        k: np.mean([metrics[k] for metrics in batch_metrics_np]) for k in batch_metrics_np[0]\n",
    "    }\n",
    "    return state, epoch_metrics_np\n",
    "\n",
    "def eval_batch_(state, images, labels, num_classes=10):\n",
    "    \"\"\"\n",
    "    Evaluation step: https://flax.readthedocs.io/en/latest/getting_started.html#evaluation-step\n",
    "    \"\"\"\n",
    "    logits = state.apply_fn(\n",
    "        {\"params\": state.params, \"batch_stats\": state.batch_stats}, images, mutable=False, train=False\n",
    "    )\n",
    "    return compute_metrics(logits=logits, labels=labels, num_classes=num_classes)\n",
    "eval_batch = jax.jit(eval_batch_, static_argnums=3)\n",
    "\n",
    "def eval_model(state, dataloader, num_classes=10):\n",
    "    \"\"\"\n",
    "    Eval function: https://flax.readthedocs.io/en/latest/getting_started.html#eval-function\n",
    "    \"\"\"\n",
    "    batch_metrics = []\n",
    "    for images, labels in dataloader:\n",
    "        metrics = eval_batch(state, images, labels, num_classes=num_classes)\n",
    "        batch_metrics.append(metrics)\n",
    "    batch_metrics_np = jax.device_get(batch_metrics)\n",
    "    validation_metrics_np = {\n",
    "        k: np.mean([metrics[k] for metrics in batch_metrics_np]) for k in batch_metrics_np[0]\n",
    "    }\n",
    "    return validation_metrics_np[\"loss\"], validation_metrics_np[\"accuracy\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill in the TODO below. Note that we use the Batch Norm here and need to get the batch_norm params separately.\n",
    "\n",
    "*HINT: Read the link in the code below for more information*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_state(rng, model, learning_rate, momentum):\n",
    "    \"\"\"\n",
    "    Create train state: https://flax.readthedocs.io/en/latest/getting_started.html#create-train-state\n",
    "    \"\"\"\n",
    "    variables = model.init(rng, jnp.ones(CIFAR10_INPUT_SHAPE, dtype=model.dtype), train=False)\n",
    "    # TODO: get params and batch_state from variables\n",
    "    # hint: Check the documentation of model.init, and find out how to use it\n",
    "    params, batch_stats = ... , ...\n",
    "    tx = optax.sgd(learning_rate, momentum)\n",
    "    state = TrainState.create(apply_fn=model.apply, params=params, tx=tx, batch_stats=batch_stats)\n",
    "    return state, variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define your parameters here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __getitem__(self, key):\n",
    "        return getattr(self, key)\n",
    "    \n",
    "    def __setitem__(self, key, val):\n",
    "        setattr(self, key, val)\n",
    "        \n",
    "    def __contains__(self, key):\n",
    "        return hasattr(self, key)\n",
    "    \n",
    "    # Define What RotNet architecture to use.\n",
    "    \"\"\"\n",
    "        Note the RotNet architecture is specified as rotnetX_featY.\n",
    "        Here X is the number of CNN layers in the rotnet\n",
    "        Here Y is the layer from which we want to extract the features for creating the final model\n",
    "\n",
    "        NOTE: Y <= X. You CANNOT extract features from a layer that DOES NOT EXIST!\n",
    "    \"\"\"\n",
    "    rotnet_arch : str = \"rotnet3_feat3\"  #@param {type: \"string\"}\n",
    "\n",
    "    # Define What PredNet architecture to use.\n",
    "    \"\"\"\n",
    "        Note the PredNet Classifier (Head) architecture is specified as prednetX.\n",
    "        Here X refers to the number of convolutional layers to be added on top of the specified RotNet Backbone.\n",
    "\n",
    "        NOTE: X=0 here refers to no CNN layers in the head and only one dense layer.\n",
    "    \"\"\"\n",
    "    prednet_arch: str = \"prednet3\" #@param {type: \"string\"}\n",
    "\n",
    "    # Define Directory to Save RotNet Checkpoints\"\n",
    "    rotnet_ckpt_dir: str = \"./ckpts/rotnet\" #@param {type: \"string\"}\n",
    "    \n",
    "    # Define Directory to Save PredNet Checkpoints\n",
    "    prednet_ckpt_dir: str = \"./ckpts/prednet\" #@param {type: \"string\"}\n",
    "    \n",
    "    # -------------------------- RotNet Training Params -------------------------- #\n",
    "    # Continue to Train RotNet from rotnet_ckpt_epoch\n",
    "    rotnet_ckpt_epoch: int = 0 #@param {type: \"integer\"}\n",
    "\n",
    "    # Train RotNet for rotnet_epochs in Total\n",
    "    rotnet_epochs: int = 10 #@param {type: \"integer\"}\n",
    "    \n",
    "    # -------------------------- PredNet Training Params ------------------------- #\n",
    "    # Continue to train PredNet from prednet_ckpt_epoch\n",
    "    prednet_ckpt_epoch: int = 0 #@param {type: \"integer\"}\n",
    "    \n",
    "    # Train PredNet for prednet_epochs in Total\n",
    "    prednet_epochs: int = 10 #@param {type: \"integer\"}\n",
    "\n",
    "    # -------------------------- General Training Params ------------------------- #\n",
    "    # Batch Size Per Process\"\n",
    "    batch_size: int = 128 #@param {type: \"integer\"}\n",
    "    # Number of Data Loading Workers\n",
    "    workers: int = 4 #@param {type: \"integer\"}\n",
    "    # Learning Rate of the Optimizer\n",
    "    lr: float = 1e-3 #@param {type: \"float\"}\n",
    "    # Momentum of the Optimizer\n",
    "    momentum: float = 0.9 #@param {type: \"float\"}\n",
    "    # Print Model and Params Info\n",
    "    verbose: bool = False #@param {type: \"boolean\"}\n",
    "\n",
    "    # ------------------ Control Gradients and Type of Training ------------------ #\n",
    "    # NOTE: To get an untrained RotNet set rotnet_epochs to 0\n",
    "    # Disable Gradient Flow in RotNet if Set to True\n",
    "    no_grad: bool = True #@param {type: \"boolean\"}\n",
    "\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing before training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- Generate JAX Random Number Key ---------------------- #\n",
    "rng = jax.random.PRNGKey(0)\n",
    "print(\"Random Key Generated\")\n",
    "\n",
    "# -------------------------- Create the RotNet Model ------------------------- #\n",
    "# Define network: https://flax.readthedocs.io/en/latest/getting_started.html#define-network\n",
    "# TODO: In order to construct a RotNet, you need to fill in all the TODOs in RotNet.py. Check the above link for hints.\n",
    "rotnet_model = rotnet_constructor(args.rotnet_arch)\n",
    "print(\"Network Defined\")\n",
    "if args.verbose:\n",
    "    print(nn.tabulate(rotnet_model, rng)(jnp.ones(CIFAR10_INPUT_SHAPE), False))\n",
    "\n",
    "# ------------------------- Load the CIFAR10 Dataset ------------------------- #\n",
    "# Loading data: https://flax.readthedocs.io/en/latest/getting_started.html#loading-data\n",
    "# NOTE: Choose batch_size and workers based on system specs.\n",
    "# NOTE: This dataloader requires pytorch to load the datset for convenience.\n",
    "# TODO: In order to create the dataset, you will need to implement rotate_image function in utils.py. Check the above link for hints.\n",
    "loaders = load_data(batch_size=args.batch_size, workers=args.workers)\n",
    "train_loader, validation_loader, test_loader, rot_train_loader, rot_validation_loader, rot_test_loader = loaders\n",
    "print(\"Data Loaded\")\n",
    "\n",
    "# --- Create the Train State Abstraction (see documentation in link below) --- #\n",
    "# Create train state: https://flax.readthedocs.io/en/latest/getting_started.html#create-train-state\n",
    "rotnet_state, rotnet_variables = create_train_state(rng, rotnet_model, args.lr, args.momentum)\n",
    "print(\"Train State Created\")\n",
    "\n",
    "# ----------------- Specify the Directory to Save Checkpoints ---------------- #\n",
    "rotnet_ckpt_dir = args.rotnet_ckpt_dir\n",
    "if not os.path.exists(rotnet_ckpt_dir):\n",
    "    os.makedirs(rotnet_ckpt_dir)\n",
    "    print(\"RotNet Checkpoint Directory Created\")\n",
    "else:\n",
    "    print(\"RotNet Checkpoint Directory Found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a RotNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Load Existing Checkpoint of RotNet -------------------- #\n",
    "if args.rotnet_ckpt_epoch > 0:\n",
    "    # TODO: Load pre-trained model if required\n",
    "    # hint: Check out flax.training.checkpoints.restore_checkpoint function\n",
    "    rotnet_state = ...\n",
    "    print(\"RotNet Checkpoint Loaded\")\n",
    "\n",
    "# ----------------------------- Train the RotNet ----------------------------- #\n",
    "print(\"Starting RotNet Training Loop\")\n",
    "\n",
    "train_acc = []\n",
    "train_loss = []\n",
    "valid_acc = []\n",
    "valid_loss = []\n",
    "test_acc =[]\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(args.rotnet_ckpt_epoch + 1, args.rotnet_epochs + 1)):\n",
    "    # ------------------------------- Training Step ------------------------------ #\n",
    "    # Training step: https://flax.readthedocs.io/en/latest/getting_started.html#training-step\n",
    "    # TODO: Use train_epoch defined above to train a RotNet \n",
    "    rotnet_state, train_epoch_metrics = ...\n",
    "\n",
    "    # Print training metrics every epoch\n",
    "    print(\n",
    "        f\"train epoch: {epoch}, \\\n",
    "        loss: {train_epoch_metrics['loss']:.4f}, \\\n",
    "        accuracy:{train_epoch_metrics['accuracy']*100:.2f}%\"\n",
    "    )\n",
    "\n",
    "    # ------------------------------ Evaluation Step ----------------------------- #\n",
    "    # Evaluation step: https://flax.readthedocs.io/en/latest/getting_started.html#evaluation-step\n",
    "    # TODO: Use eval_model defined aboove to get validation loss and accuracy\n",
    "    validation_loss, validation_accuracy = ...\n",
    "    \n",
    "    # Print validation metrics every epoch\n",
    "    print(f\"validation loss: {validation_loss:.4f}, validation accuracy:{validation_accuracy*100:.2f}%\\n\")\n",
    "\n",
    "    # ---------------------------- Saving Checkpoints ---------------------------- #\n",
    "    # ---- https://flax.readthedocs.io/en/latest/guides/use_checkpointing.html --- #\n",
    "    checkpoints.save_checkpoint(\n",
    "        ckpt_dir=rotnet_ckpt_dir, target=rotnet_state, step=epoch, overwrite=True, keep=args.rotnet_epochs\n",
    "    )\n",
    "\n",
    "    train_acc.append(train_epoch_metrics['accuracy'])\n",
    "    train_loss.append(train_epoch_metrics['loss'])\n",
    "    valid_acc.append(validation_accuracy)\n",
    "    valid_loss.append(validation_loss)\n",
    "\n",
    "    # Print test metrics every nth epoch\n",
    "    if epoch % 5 == 0:\n",
    "        _, test_accuracy = eval_model(rotnet_state, rot_test_loader, num_classes=4)\n",
    "        print(\"====================\")\n",
    "        print(f\"test_accuracy: {test_accuracy*100:.2f}%\")\n",
    "        print(\"====================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(train_acc)), train_acc, color = 'blue')\n",
    "plt.plot(np.arange(len(valid_acc)), valid_acc, color = 'green')\n",
    "plt.title(\"Training and Validation Accuracy vs epochs\")\n",
    "plt.xlabel(\"number of epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend([\"Training_Accuracy\", \"Validation_Accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(train_loss)), train_loss, color = 'blue')\n",
    "plt.plot(np.arange(len(valid_loss)), valid_loss, color = 'green')\n",
    "plt.title(\"Training and Validation Loss vs epochs\")\n",
    "plt.xlabel(\"number of epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend([\"Training_Loss\", \"Validation_Loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Surgery:\n",
    "\n",
    "Transfer the rotnet as a backbone into the PredNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- https://flax.readthedocs.io/en/latest/guides/transfer_learning.html --- #\n",
    "# ----------------------------- Extract Backbone ----------------------------- #\n",
    "def extract_submodule(model):\n",
    "    feature_extractor = model.features.clone()\n",
    "    # TODO: extract the variables from the feature_extractor\n",
    "    # hint: checkout https://flax.readthedocs.io/en/latest/guides/transfer_learning.html#extracting-a-submodule\n",
    "    variables = ...\n",
    "    return feature_extractor, variables\n",
    "\n",
    "backbone_model, backbone_model_variables = nn.apply(extract_submodule, rotnet_model)(rotnet_variables)\n",
    "\n",
    "# ------------------------- Create the Prednet Model ------------------------- #\n",
    "# TODO: Construct your PredNet. Please take a look at the TODOs in PredNet.py\n",
    "# hint: Check out how the RotNet was defined above\n",
    "prednet_model = ...\n",
    "\n",
    "# ----------------------- Extract Variables and Params ----------------------- #\n",
    "prednet_variables   = prednet_model.init(rng, jnp.ones(CIFAR10_INPUT_SHAPE), train=False)\n",
    "prednet_params      = prednet_variables['params']\n",
    "prednet_batch_stats = prednet_variables['batch_stats']\n",
    "\n",
    "# --------------------- Transfer the Backbone Parameters --------------------- #\n",
    "prednet_params              = prednet_params.unfreeze()\n",
    "prednet_params['backbone']  = backbone_model_variables['params']\n",
    "prednet_params              = freeze(prednet_params)\n",
    "\n",
    "if not args.no_grad:\n",
    "    prednet_batch_stats              = prednet_batch_stats.unfreeze()\n",
    "    prednet_batch_stats['backbone']  = backbone_model_variables['batch_stats']\n",
    "    prednet_batch_stats              = freeze(prednet_batch_stats)\n",
    "\n",
    "# -------------------------- Define How to Backprop -------------------------- #\n",
    "if args.no_grad:\n",
    "    # TODO: Freeze layers with optax.multi_transform for the mode\n",
    "    ############### Your Code Starts Here ###################\n",
    "    # hint: Check out https://flax.readthedocs.io/en/latest/guides/transfer_learning.html#optax-multi-transform\n",
    "\n",
    "    # Fill the 'frozen' value here:\n",
    "    partition_optimizers = {'trainable': optax.sgd(args.lr, args.momentum), 'frozen': ... }\n",
    "    \n",
    "    # Fill in the assigning of params as frozen.\n",
    "    prednet_param_partitions = ...\n",
    "    \n",
    "\n",
    "    tx = optax.multi_transform(partition_optimizers, prednet_param_partitions)\n",
    "    \n",
    "    ############### Your Code Ends Here ###################\n",
    "\n",
    "    # ---------------- Visualize param_partitions to double check ---------------- #\n",
    "    if args.verbose:\n",
    "        flat = list(traverse_util.flatten_dict(prednet_param_partitions).items())\n",
    "        freeze(traverse_util.unflatten_dict(dict(flat[:2] + flat[-2:])))\n",
    "        \n",
    "else:\n",
    "    tx = optax.sgd(args.lr, args.momentum)\n",
    "    \n",
    "# ---------------------- Create Train State for PredNet ---------------------- #\n",
    "# TODO: Create Train State for PredNet\n",
    "# hint: We also did it for RotNet above.\n",
    "prednet_state = ...\n",
    "\n",
    "# ----------------- Specify the Directory to Save Checkpoints ---------------- #\n",
    "prednet_ckpt_dir = args.prednet_ckpt_dir\n",
    "if not os.path.exists(prednet_ckpt_dir):\n",
    "    os.makedirs(prednet_ckpt_dir)\n",
    "    print(\"PredNet Checkpoint Directory Created\")\n",
    "else:\n",
    "    print(\"PredNet Checkpoint Directory Found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a PredNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Load Existing Checkpoint of PredNet ------------------- #\n",
    "if args.prednet_ckpt_epoch > 0:\n",
    "    prednet_state = checkpoints.restore_checkpoint(\n",
    "        ckpt_dir=prednet_ckpt_dir, target=prednet_state, step=args.prednet_ckpt_epoch\n",
    "    )\n",
    "    print(\"PredNet Checkpoint Loaded\")\n",
    "\n",
    "# ----------------------------- Train the PredNet ---------------------------- #\n",
    "print(\"Starting PredNet Training Loop\")\n",
    "\n",
    "train_acc = []\n",
    "train_loss = []\n",
    "valid_acc = []\n",
    "valid_loss = []\n",
    "test_acc =[]\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(args.prednet_ckpt_epoch + 1, args.prednet_epochs + 1)):\n",
    "    # ------------------------------- Training Step ------------------------------ #\n",
    "    # Training step: https://flax.readthedocs.io/en/latest/getting_started.html#training-step\n",
    "    # TODO: Use train_epoch defined above to train. \n",
    "    prednet_state, train_epoch_metrics = ...\n",
    "\n",
    "    # Print training metrics every epoch\n",
    "    print(\n",
    "        f\"train epoch: {epoch}, \\\n",
    "        loss: {train_epoch_metrics['loss']:.4f}, \\\n",
    "        accuracy:{train_epoch_metrics['accuracy']*100:.2f}%\"\n",
    "    )\n",
    "\n",
    "    # ------------------------------ Evaluation Step ----------------------------- #\n",
    "    # Evaluation step: https://flax.readthedocs.io/en/latest/getting_started.html#evaluation-step\n",
    "    # TODO: Use eval_model defined above to get validation loss and accuracy\n",
    "    validation_loss, validation_accuracy = ...\n",
    "    \n",
    "    # Print validation metrics every epoch\n",
    "    print(f\"validation loss: {validation_loss:.4f}, validation accuracy:{validation_accuracy*100:.2f}%\\n\")\n",
    "\n",
    "    # ---------------------------- Saving Checkpoints ---------------------------- #\n",
    "    # ---- https://flax.readthedocs.io/en/latest/guides/use_checkpointing.html --- #\n",
    "    checkpoints.save_checkpoint(\n",
    "        ckpt_dir=prednet_ckpt_dir, target=prednet_state, step=epoch, overwrite=True, keep=args.prednet_epochs\n",
    "    )\n",
    "\n",
    "    train_acc.append(train_epoch_metrics['accuracy'])\n",
    "    train_loss.append(train_epoch_metrics['loss'])\n",
    "    valid_acc.append(validation_accuracy)\n",
    "    valid_loss.append(validation_loss)\n",
    "    # Print test metrics every nth epoch\n",
    "    if epoch % 5 == 0:\n",
    "        _, test_accuracy = eval_model(prednet_state, test_loader, num_classes=10)\n",
    "        print(\"====================\")\n",
    "        print(f\"test_accuracy: {test_accuracy*100:.2f}%\")\n",
    "        print(\"====================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(train_acc)), train_acc, color = 'blue')\n",
    "plt.plot(np.arange(len(valid_acc)), valid_acc, color = 'green')\n",
    "plt.title(\"Training and Validation Accuracy vs epochs\")\n",
    "plt.xlabel(\"number of epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend([\"Training_Accuracy\", \"Validation_Accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(train_loss)), train_loss, color = 'blue')\n",
    "plt.plot(np.arange(len(valid_loss)), valid_loss, color = 'green')\n",
    "plt.title(\"Training and Validation Loss vs epochs\")\n",
    "plt.xlabel(\"number of epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend([\"Training_Loss\", \"Validation_Loss\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('rotnet_jax')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cd06a5807f0071989d42e2637ffa6894bf1f8c990e09f9890c8effe19fbb4b80"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
